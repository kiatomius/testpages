---
title: Summary
nav_include: 5
---

## 1. Results
Using the dataset presented by Cresci-2017, we devised a number of classification models based on twitter user database, and we sought to improve the results by incorporating twitter/text analysis. The summary of the performance of our models are presented as below:

### 1.1 Prediction based on User Data

| Model               | Train score | Test score #1 | Test score #3 |
|---------------------|-------------|---------------|---------------|
| Decision Tree       | 98.9%       | 69.9%         | 77.7%         |
| Bagging             | 100%        | 84.8%         | 54.4%         |
| Boosting            | 100%        | 88.0%         | 52.9%         |
| Random Forest       | 100%        | 84.1%         | 77.5%         |
| Logistic Regression | 97.6%       | 69.0%         | 51.7%         |
| KNN                 | 97.6%       | 64.4%         | 67.2%         |

### 1.2 Prediction based on User Data + Text

| Model               | Test score #3(dropNaN) | Test score #3(linear imputation) |
|---------------------|---------------|---------------|
| Decision Tree       | 95.1%         | 49.0%         |
| Bagging             | 95.1%         | 94.1%         |
| Boosting            | 95.1%         | 82.4%         |
| Random Forest       | 96.7%         | 83.2%         |
| Logistic Regression | 95.6%         | 95.7%         |
| KNN                 | 91.2%         | 92.9%         |


### 1.3 Summary & Thoughts
We were able to achieve the target accuracy levels that we had originally set (based on Yang et al.). By combining our intial analysis with text analysis, we also obtain higher accurary levels, closer to those achieved by BotOrNot? (Botometer), even approaching the levels of unsupervised models that Cresci-2017 had tested. While we were pleased to achieve our initial targets, a number of questions arose that we were not fully able to comprehend at this time, and we wish to hereby state for future discussions.

The first question that we faced was the unique way through which Cresci-2017 had constructed the training and testing dataset, where the 'bot' part of the data for training and testing set originate from completely different sources. We initially combined all sources of bot-data, shuffled them, and split them into training/testing sets,as just we approached this topic in CS109. However this approach resulted in inexplicably high testing accuracy score (consistently in 98-99% range), and we revised-back our approach to Cresci-2017 model in order to carry out apple-to-apple comparison against the relevant scores. While this turned out to be an enormously fruitful exercise, we were still not able to fully comprehend the flaws in our intial train-test split approach and how that produced such high scores.

Another major question that remained was that a large number of users in Cresci-2017 dataset had no twitter data, therefore we were forced to drastically reduce the sample size for the text/sentiment analysis. We were not able to comprehend this situation, since it seems unusual that twitter data and user data were separately collected by the researchers in Cresci-2017. There is also a more troubling implication, that, the missingness of the twitter-data may not be random, and this may be causing significant biases to our models, in ways that would make them ineffective in predicting the 'real' twitter dataset.

A final question which we faced was that, we did not understand why some of the accuracy scores presented in the literature were so low (Yang et. al 's 51% is almost as good as a coin-flip). This may not have stemmed from the inferior construction of their models, possibly due to sampling of the dataset upon which the models were trained and tested. While our performance was good, we may also fall into this trap, and this point require a careful evaluation. A sobering implication to derive from here may be that, correct sampling of twitter data is inherently difficult, as the true-population size of the twitter data is so exponentially large, and however hard we seek to devise optimal models, they would be biased towards the data that they were trained on. 

Our attempt to make a progress upon this concluding thought is presented in the following Future Scope section.

### Thoughts on twitter
* **Humans are not good at detecting bots!** You may assume that you can verify whether a tweet is generated by bot or not. Not so fast! Cresci-2017 shows that accuracy of human annotators range from 69% to 82%. Establishing the 'ground truth' is not as easy as we might think! But then, how else do we establish the ground truth? Twitter has 'verified' column which ensures that account is from a verified human-being. We had high hopes that this will become a powerful predictor but it turned out that hardly anyone is verified! Maybe it's time for twitter to change its registration process?