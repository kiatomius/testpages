---
title: Summary
nav_include: 5
---

## 1. Results
Using the dataset presented by Cresci-2017, we devised a number of classification models based on twitter user database, and we sought to improve the results by incorporating twitter/text analysis. The summary of the performance of our models are presented as below:

### 1.1 Prediction based on User Data

| Model               | Train score | Test score #1 | Test score #3 |
|---------------------|-------------|---------------|---------------|
| Decision Tree       | 98.9%       | 69.9%         | 77.7%         |
| Bagging             | 100%        | 84.8%         | 54.4%         |
| Boosting            | 100%        | 88.0%         | 52.9%         |
| Random Forest       | 100%        | 84.1%         | 77.5%         |
| Logistic Regression | 97.6%       | 69.0%         | 51.7%         |
| KNN                 | 97.6%       | 64.4%         | 67.2%         |

### 1.2 Prediction based on User Data + Text

| Model               | Train score | Test score #1 | Test score #3 |
|---------------------|-------------|---------------|---------------|
| Decision Tree       | XXX%        | NA            | 77.7%         |
| Bagging             | XXX%        | NA            | 54.4%         |
| Boosting            | XXX%        | NA            | 52.9%         |
| Random Forest       | XXX%        | NA            | 77.5%         |
| Logistic Regression | XXX%        | NA            | 51.7%         |
| KNN                 | XXX%        | NA            | 67.2%         |


### 1.3 Summary & Thoughts
We were able to achieve the target accuracy levels that we had originally set (based on Yang et al.) through our models. By combining the text analysis, we also achieved the higher target accurary levels achieved by BotOrNot? (Botometer), and even approach the levels achieved by unsupervised models that Cresci-2017 had presented.  While we were pleased to achieve our initial targets, a number of questions rose that we were not fully able to comprehend at this time, and we wish to further investigate beyond this project.

The first question that we faced was the unique way in which Cresci-2017 had constructed the training and testing dataset, where the 'bot' part of the data for training set and testing set stem from completely different sources. We originally combined all the bot-data, shuffled them, and split them into training/testing sets, however this resulted in unusually high testing accuracy score (consistently in 98-99% range), and we had revised the approach to Cresci-2017 methodology to carry out apple-to-apple comparison. While this turned out to be an enormously fruitful exercise in the end, we were able to fully comprehend the flaw in our original train-test split approach and how that produced these high scores.

Another major question that remained was that a large number of users in Cresci-2017 dataset had no twitter data, therefore we had to drastically reduce the sample size. We were not able to comprehend why this situation exists, since it seems unlikely that twitter data and user data were separately collected by Cresci-2017. Moreover, more troubling implication was that, if the missing twitter-data were not random, this could create significant biases in our data selection, skewing our models in ways that may not allow them to perform well in 'real' dataset.

A final question was, we seriously questioned why some of the accuracy scores presented in the literature were so low (Yang et. al 's 50% is almost as good as a coin-flip). This may not have stemmed from the inferior construction of these models in relevant literature, but their models, in spite of train-test splits, were still highly biased to their unique dataset that they obtained, and as the sample data evolves, models quickly become outdated. While our performance was good, we may also fall into this trap, and require a careful evaluation.

A sobering implication to derive from here may be that, correct sampling of twitter data is inherently difficult, for population size is exponentially large, and however hard we seek to devise optimal models, they would be biased towards the data that they were trained on. 

Our attempt to make a progress upon this concluding thought in presented in the following Future Scope section.

### thoughts on twitter
* Humans are not good at detecting bots!: you may assume that you can verify whether a tweet is generated by bot or not. Not so fast! Cresci-2017 shows that accuracy of human annotators range from 69% to 82%. Establishing the 'ground truth' is not as easy as we might think! But then, how else do we establish the ground truth? Twitter has 'verified' column which ensures that account is from a verified human-being. We had high hopes that this will become a powerful predictor but it turned out that hardly anyone is verified! Maybe it's time for twitter to change its registration process?